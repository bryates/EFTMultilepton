import sys
import os
# import glob
# import subprocess
import argparse
import logging
import datetime
import shutil

from EFTMultilepton.TemplateMakers.LvlFormatter import LvlFormatter

# A python script for generating the 'inputfiles__*.txt' files which are used to run the histogram
#   making part of the analysis workflow. The inputs should be root files generated by the analysis
#   tree maker. This acts as a replacement to the 'justmakefilelists.C' script.

TIMESTAMP1 = datetime.datetime.now().strftime('%Y_%m_%d')
TIMESTAMP2 = datetime.datetime.now().strftime('%Y%m%d_%H%M')
TIMESTAMP3 = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

USER_NAME = os.environ['USER']
USER_DIR = os.path.expanduser('~')
HOME_DIR = os.getcwd()
HADOOP_DIR = "/hadoop/store/user"

# TMPSCRATCH_DIR = os.path.expandvars("/tmpscratch/users/$USER")
TMPSCRATCH_DIR = os.path.join("/tmpscratch/users",USER_NAME)
if not os.path.exists(TMPSCRATCH_DIR):
    err = "[ERROR] {user} does not have a /tmpscratch/users/$USER area setup, please set one up before using this script!".format(user=USER_NAME)
    raise RuntimeError(err)

INPUTFILES_DIR = os.path.join(TMPSCRATCH_DIR,"analysisWorkflow/inputFiles")
if not os.path.exists(INPUTFILES_DIR):
    os.makedirs(INPUTFILES_DIR)

# frmt = LvlFormatter()
# logging.getLogger().setLevel(logging.DEBUG)   # Modify the root logger

# # Configure logging to also output to stdout
# console = logging.StreamHandler()
# console.setLevel(logging.INFO)
# console.setFormatter(frmt)
# logging.getLogger('').addHandler(console)   # Also the root logger

frmt = LvlFormatter()
# Configure logging to also output to stdout
console = logging.StreamHandler()
console.setLevel(logging.INFO)
console.setFormatter(frmt)

# Get the root logger and configure it
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
logger.addHandler(console)

class Sample(object):
    '''
        A basic container class for associating one or multiple directories
            that contain root files with a particular sample. Also provides
            methods for writing this information to a .txt file that can be
            used in the histogram making step of the analysis workflow
    '''
    def __init__(self,name,dirs=[]):
        self.logger = logging.getLogger(__name__)
        self.__name = name
        self.__dirs = []
        for d in dirs: self.addDirectory(d)

    # Return the name that this sample has been given
    def name(self):
        return self.__name

    # Returns a list of all directories that have been associated with this sample
    def list(self):
        return self.__dirs

    # Write the file paths of all root files from all attached directories to txt file
    def write(self,out_dir,sub_index=False,skip_copy=False):
        '''
            out_dir   - Specifies the output directory name to be used
            sub_index - In addition to the normal inputfile, will create
                        an additional inputfile per added sample directory
            skip_copy - Will skip copying any inputfiles to the output directory
        '''
        outf = 'inputfiles__{name}.txt'.format(name=self.name())
        file_dict = {}
        to_copy = []
        lst = []

        file_dict[outf] = []
        to_copy.append(outf)
        for tdir in self.list():
            if not os.path.exists(tdir):
                self.logger.info("Skipping unknown directory {dir}".format(dir=tdir))
                continue
            self.logger.info("Adding files from {dir}".format(dir=tdir))
            sub_lst = []    # For sub indexing if needed
            for fn in os.listdir(tdir):
                fpath = os.path.join(tdir,fn)
                if not os.path.isfile(fpath):
                    continue
                h,t = fn.rsplit('.',1)
                if t == 'root':
                    lst.append(fpath)
                    sub_lst.append(fpath)
            file_dict[outf].extend(sub_lst)
            if sub_index:
                sub_index_name = tdir.rsplit("/",1)[1]
                sub_index_fn = "inputfiles__{name}.txt".format(name=sub_index_name)
                if not file_dict.has_key(sub_index_fn):
                    file_dict[sub_index_fn] = []
                file_dict[sub_index_fn].extend(sub_lst)
                if sub_index_fn in to_copy:
                    raise RuntimeError("Duplicate output file name found! {}".format(sub_index_fn))
                to_copy.append(sub_index_fn)
        for fn in to_copy:
            if not file_dict.has_key(fn):
                raise RuntimeError("Missing dict entry for {}".format(fn))
            file_lst = file_dict[fn]
            if fn == outf:
                self.logger.info("Creating file: {fn} (files: {nfiles})".format(fn=fn,nfiles=len(file_lst)))
            else:
                self.logger.info("Creating Sub-Indexed file: {fn} (files: {nfiles})".format(fn=fn,nfiles=len(file_lst)))
            with open(fn,'w') as f:
                for fpath in file_lst:
                    f.write(fpath+"\n")
        self.logger.info("Total Found files: {0:d}".format(len(file_dict[outf])))   # TODO: Find a better way to track total files
        if not skip_copy:
            for fn in to_copy:
                self.logger.info("Copying {} to {}".format(fn,out_dir))
                shutil.copy(fn,out_dir)

    # Add a single directory containing root files to be associated with this sample
    def addDirectory(self,*args):
        if len(args) == 0:
            return
        elif len(args) == 1:
            d = args[0]
        else:
            d = os.path.join(*args)
        if d in self.list():
            return
        self.__dirs.append(d)

####################################################################################################
# Define 'historic' groups of samples used in the analysis
# TODO: Figure out a better way to document/record the groups
####################################################################################################

# Still need to double check that these re-create the inputfiles verbatim
def legacy_geoff_samples(private_sgnl,central_sgnl,central_bkgd,data):
    logger.info("include private_sgnl: {}".format(private_sgnl))
    logger.info("include central_sgnl: {}".format(central_sgnl))
    logger.info("include central_bkgd: {}".format(central_bkgd))
    logger.info("include data: {}".format(data))

    # Central signal samples
    # Note: There isn't a central tHq sample
    path = "awightma/geoff_lobster_outputs/lobster_trees__EFT_test_27_2_19_central"
    ttZ_central = Sample('ttZ')
    ttZ_central.addDirectory(HADOOP_DIR,path,'ttZ')

    ttW_central = Sample('ttW')    
    ttW_central.addDirectory(HADOOP_DIR,path,'ttW')

    ttH_central = Sample('ttH')
    ttH_central.addDirectory(HADOOP_DIR,path,'ttH')

    path = "awightma/geoff_lobster_outputs/lobster_trees__EFT_test_tZq_rmHiggs_8_7_19"
    tZq_central = Sample('tZq')
    tZq_central.addDirectory(HADOOP_DIR,path,'tZq')

    # Central bkgd samples
    path = "awightma/geoff_lobster_outputs/lobster_trees__EFT_test_20_2_19_central_a"
    ttGJets_central = Sample('ttGJets')
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets')

    ZZZ_central = Sample('ZZZ')
    ZZZ_central.addDirectory(HADOOP_DIR,path,'ZZZ')

    WZZ_central = Sample('WZZ')
    WZZ_central.addDirectory(HADOOP_DIR,path,'WZZ')

    WZ_central = Sample('WZ')
    WZ_central.addDirectory(HADOOP_DIR,path,'WZ')
    
    WWZ_central = Sample('WWZ')
    WWZ_central.addDirectory(HADOOP_DIR,path,'WWZ')
    
    WWW_central = Sample('WWW')
    WWW_central.addDirectory(HADOOP_DIR,path,'WWW')

    path = "awightma/geoff_lobster_outputs/lobster_trees__EFT_test_20_2_19_central_b"
    ZZ_central = Sample('ZZ')
    ZZ_central.addDirectory(HADOOP_DIR,path,'ZZ')
    
    WW_central = Sample('WW')
    WW_central.addDirectory(HADOOP_DIR,path,'WW')
    
    # Data samples
    path = "awightma/geoff_lobster_outputs/lobster_trees__EFT_test_20_2_19_data_take2"
    SingleMuon_data = Sample('SingleMuon')
    SingleMuon_data.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017B')
    SingleMuon_data.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017C')
    SingleMuon_data.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017D')
    SingleMuon_data.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017E')
    SingleMuon_data.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017F')

    SingleElectron_data = Sample('SingleElectron')
    SingleElectron_data.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017B')
    SingleElectron_data.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017C')
    SingleElectron_data.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017D')
    SingleElectron_data.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017E')
    SingleElectron_data.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017F')

    MuonEG_data = Sample('MuonEG')
    MuonEG_data.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017B')
    MuonEG_data.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017C')
    MuonEG_data.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017D')
    MuonEG_data.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017E')
    MuonEG_data.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017F')

    DoubleMuon_data = Sample('DoubleMuon')
    DoubleMuon_data.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017B')
    DoubleMuon_data.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017C')
    DoubleMuon_data.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017D')
    DoubleMuon_data.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017E')
    DoubleMuon_data.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017F')

    DoubleEG_data = Sample('DoubleEG')
    DoubleEG_data.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017B')
    DoubleEG_data.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017C')
    DoubleEG_data.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017D')
    DoubleEG_data.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017E')
    DoubleEG_data.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017F')

    # Private EFT samples
    path = "awightma/geoff_lobster_outputs/lobster_trees__EFT_test_EFTsamps_first_Round5_6_7_19"
    tllq_priv = Sample('tllq_multidim')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_batch1')
    
    ttlnu_priv = Sample('ttlnu_multidim')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_batch1')
    
    ttll_priv = Sample('ttll_multidim')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_batch1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_batch2')
    
    ttH_priv = Sample('ttH_multidim')
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_batch1')

    tHq_priv = Sample('tHq_multidim')
    tHq_priv.addDirectory(HADOOP_DIR,path,'tHq_multidim_batch1')

    samples = []

    if private_sgnl: samples.extend([tllq_priv,ttlnu_priv,ttll_priv,ttH_priv,tHq_priv])
    if central_sgnl: samples.extend([ttZ_central,ttW_central,ttH_central,tZq_central])
    if central_bkgd: samples.extend([ttGJets_central,ZZZ_central,WZZ_central,WZ_central,WWZ_central,WWW_central,ZZ_central,WW_central])
    if data: samples.extend([SingleMuon_data,SingleElectron_data,MuonEG_data,DoubleMuon_data,DoubleEG_data])

    return samples

# 'Current' samples we want to use over the legacy geoff samples
def private_samples():
    ttH_priv   = Sample('ttH_multidim')
    tHq_priv   = Sample('tHq_multidim')
    tllq_priv  = Sample('tllq_multidim')
    ttll_priv  = Sample('ttll_multidim')
    ttlnu_priv = Sample('ttlnu_multidim')
    
    samples = []

    path = 'awightman/analysisTrees/private_sgnl_2019_10_09/v1'
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b1')
    tHq_priv.addDirectory(HADOOP_DIR,path,'tHq_multidim_b1')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b2')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b2')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b1')

    samples.extend([tllq_priv,ttlnu_priv,ttll_priv,ttH_priv,tHq_priv])

    return samples

# MC based samples for anatest25
def anatest25_samples():
    ttZ_central = Sample('ttZ')
    ttW_central = Sample('ttW')    
    ttH_central = Sample('ttH')
    tZq_central = Sample('tZq')
    tHq_central = Sample('tHq')

    ttGJets_central = Sample('ttGJets')
    ZZZ_central = Sample('ZZZ')
    WZZ_central = Sample('WZZ')
    WWZ_central = Sample('WWZ')
    WWW_central = Sample('WWW')
    ZZ_central  = Sample('ZZ')
    WZ_central  = Sample('WZ')
    WW_central  = Sample('WW')

    ttH_priv   = Sample('ttH_multidim')
    tHq_priv   = Sample('tHq_multidim')
    ttll_priv  = Sample('ttll_multidim')
    tllq_priv  = Sample('tllq_multidim')
    ttlnu_priv = Sample('ttlnu_multidim')

    samples = []

    path = 'awightma/analysisTrees/central_sgnl_2019_10_11/v1'
    ttH_central.addDirectory(HADOOP_DIR,path,'ttH')
    tHq_central.addDirectory(HADOOP_DIR,path,'tHq')
    ttW_central.addDirectory(HADOOP_DIR,path,'ttW')
    ttZ_central.addDirectory(HADOOP_DIR,path,'ttZ')
    tZq_central.addDirectory(HADOOP_DIR,path,'tZq')

    samples.extend([ttH_central,tHq_central,ttW_central,ttZ_central,tZq_central])

    path = 'awightma/analysisTrees/central_bkgd_2019_10_12/v1'
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets')
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets_ext')
    ZZZ_central.addDirectory(HADOOP_DIR,path,'ZZZ')
    WZZ_central.addDirectory(HADOOP_DIR,path,'WZZ')
    WWZ_central.addDirectory(HADOOP_DIR,path,'WWZ')
    WWW_central.addDirectory(HADOOP_DIR,path,'WWW')
    ZZ_central.addDirectory(HADOOP_DIR,path,'ZZ')
    WZ_central.addDirectory(HADOOP_DIR,path,'WZ')
    WW_central.addDirectory(HADOOP_DIR,path,'WW')

    samples.extend([
        ttGJets_central,
        ZZZ_central,WZZ_central,WWZ_central,WWW_central,
        ZZ_central,WZ_central,WW_central
    ])

    path = 'awightma/analysisTrees/private_sgnl_2019_10_11/v1'
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b1')
    tHq_priv.addDirectory(HADOOP_DIR,path,'tHq_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b2')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b2')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b1')
    
    samples.extend([ttH_priv,tHq_priv,ttll_priv,tllq_priv,ttlnu_priv])

    return samples

# MC based samples for anatest26
def anatest26_samples():
    ttZ_central = Sample('ttZ')
    ttW_central = Sample('ttW')    
    ttH_central = Sample('ttH')
    tZq_central = Sample('tZq')
    tHq_central = Sample('tHq')

    ttGJets_central = Sample('ttGJets')
    ZZZ_central = Sample('ZZZ')
    WZZ_central = Sample('WZZ')
    WWZ_central = Sample('WWZ')
    WWW_central = Sample('WWW')
    ZZ_central  = Sample('ZZ')
    WZ_central  = Sample('WZ')
    WW_central  = Sample('WW')

    ttH_priv   = Sample('ttH_multidim')
    tHq_priv   = Sample('tHq_multidim')
    ttll_priv  = Sample('ttll_multidim')
    tllq_priv  = Sample('tllq_multidim')
    ttlnu_priv = Sample('ttlnu_multidim')

    samples = []

    path = 'awightma/analysisTrees/central_sgnl_2019_10_11/v1'
    ttH_central.addDirectory(HADOOP_DIR,path,'ttH')
    tHq_central.addDirectory(HADOOP_DIR,path,'tHq')
    ttW_central.addDirectory(HADOOP_DIR,path,'ttW')
    ttZ_central.addDirectory(HADOOP_DIR,path,'ttZ')

    path = 'awightma/analysisTrees/central_bkgd_2019_10_12/v1'
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets')
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets_ext')
    ZZZ_central.addDirectory(HADOOP_DIR,path,'ZZZ')
    WZZ_central.addDirectory(HADOOP_DIR,path,'WZZ')
    WWZ_central.addDirectory(HADOOP_DIR,path,'WWZ')
    WWW_central.addDirectory(HADOOP_DIR,path,'WWW')
    ZZ_central.addDirectory(HADOOP_DIR,path,'ZZ')
    WZ_central.addDirectory(HADOOP_DIR,path,'WZ')
    WW_central.addDirectory(HADOOP_DIR,path,'WW')

    path = 'awightma/analysisTrees/private_sgnl_2019_10_11/v1'
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b1')
    tHq_priv.addDirectory(HADOOP_DIR,path,'tHq_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b2')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b1')
    
    path = 'awightma/analysisTrees/special/tllq4f_2019_10_17_fixPDF/v1'
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b2')

    path = 'awightma/analysisTrees/special/tZq_2019_10_17_fixPDF/v2'
    tZq_central.addDirectory(HADOOP_DIR,path,'tZq')

    samples.extend([ttH_central,tHq_central,ttW_central,ttZ_central,tZq_central])
    samples.extend([
        ttGJets_central,
        ZZZ_central,WZZ_central,WWZ_central,WWW_central,
        ZZ_central,WZ_central,WW_central
    ])
    samples.extend([ttH_priv,tHq_priv,ttll_priv,tllq_priv,ttlnu_priv])

    return samples

# Re-processed data with new GT and updated some central samples to 'new_pmx'
def anatest27_samples():
    SingleElectron = Sample('SingleElectron')
    SingleMuon = Sample('SingleMuon')
    DoubleMuon = Sample('DoubleMuon')
    DoubleEG   = Sample('DoubleEG')
    MuonEG     = Sample('MuonEG')

    ttZ_central = Sample('ttZ')
    ttW_central = Sample('ttW')
    ttH_central = Sample('ttH')
    tZq_central = Sample('tZq')
    tHq_central = Sample('tHq')

    ttGJets_central = Sample('ttGJets')
    ZZZ_central = Sample('ZZZ')
    WZZ_central = Sample('WZZ')
    WWZ_central = Sample('WWZ')
    WWW_central = Sample('WWW')
    ZZ_central  = Sample('ZZ')
    WZ_central  = Sample('WZ')
    WW_central  = Sample('WW')

    ttH_priv   = Sample('ttH_multidim')
    tHq_priv   = Sample('tHq_multidim')
    ttll_priv  = Sample('ttll_multidim')
    tllq_priv  = Sample('tllq_multidim')
    ttlnu_priv = Sample('ttlnu_multidim')

    samples = []

    # Data samples
    path = 'awightma/analysisTrees/data2017_2019_10_21/v1'
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017B')
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017C')
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017D')
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017E')
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017F')

    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017B')
    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017C')
    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017D')
    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017E')
    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017F')
    # SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017H')    # Has no data

    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017B')
    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017C')
    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017D')
    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017E')
    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017F')

    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017B')
    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017C')
    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017D')
    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017E')
    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017F')

    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017B')
    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017C')
    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017D')
    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017E')
    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017F')

    samples.extend([SingleElectron,SingleMuon,DoubleMuon,DoubleEG,MuonEG])

    # Updated to 'new_pmx' version (Note: The ttGJets sample failed horrifically)
    path = 'awightma/analysisTrees/special/central_ttH-WW-WZ-ttGJets_new_pmx_2019_10_21/v3'
    ttH_central.addDirectory(HADOOP_DIR,path,'ttH')
    WZ_central.addDirectory(HADOOP_DIR,path,'WZ')
    ZZ_central.addDirectory(HADOOP_DIR,path,'ZZ')

    samples.extend([ttH_central,WZ_central,ZZ_central])

    # Central samples (same as ana26)
    path = 'awightma/analysisTrees/central_sgnl_2019_10_11/v1'
    tHq_central.addDirectory(HADOOP_DIR,path,'tHq') # Excluding this from the list for now
    ttW_central.addDirectory(HADOOP_DIR,path,'ttW')
    ttZ_central.addDirectory(HADOOP_DIR,path,'ttZ')

    path = 'awightma/analysisTrees/special/tZq_2019_10_17_fixPDF/v2'
    tZq_central.addDirectory(HADOOP_DIR,path,'tZq')

    samples.extend([ttW_central,ttZ_central,tZq_central])

    path = 'awightma/analysisTrees/central_bkgd_2019_10_12/v1'
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets')
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets_ext')
    ZZZ_central.addDirectory(HADOOP_DIR,path,'ZZZ')
    WZZ_central.addDirectory(HADOOP_DIR,path,'WZZ')
    WWZ_central.addDirectory(HADOOP_DIR,path,'WWZ')
    WWW_central.addDirectory(HADOOP_DIR,path,'WWW')
    WW_central.addDirectory(HADOOP_DIR,path,'WW')

    samples.extend([ttGJets_central,ZZZ_central,WZZ_central,WWZ_central,WWW_central,WW_central])

    # Private signal samples (same as ana26)
    path = 'awightma/analysisTrees/private_sgnl_2019_10_11/v1'
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b1')
    tHq_priv.addDirectory(HADOOP_DIR,path,'tHq_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b2')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b1')
    
    path = 'awightma/analysisTrees/special/tllq4f_2019_10_17_fixPDF/v1'
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b2')

    samples.extend([ttH_priv,tHq_priv,ttll_priv,ttlnu_priv,tllq_priv])

    return samples

# 'Round6' private signal samples all produced with HanModelV4 version of dim6top model processed with old GTv6
def anatest29_samples():
    ttH_priv   = Sample('ttH_multidim')
    tHq_priv   = Sample('tHq_multidim')
    ttll_priv  = Sample('ttll_multidim')
    tllq_priv  = Sample('tllq_multidim')
    ttlnu_priv = Sample('ttlnu_multidim')

    samples = []

    path = 'awightma/analysisTrees/private_sgnl_2019_11_05/v1'
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b1')
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b2')
    tHq_priv.addDirectory(HADOOP_DIR,path,'tHq_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b2')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b1')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b2')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b2')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b3')

    samples.extend([ttH_priv,tHq_priv,ttll_priv,tllq_priv,ttlnu_priv])

    return samples

# Samples produced from the same mAOD as a29, but without the lobster job crashes at treeMaking or histMaking level
def anatest31_samples():
    ttH_priv   = Sample('ttH_multidim')
    tHq_priv   = Sample('tHq_multidim')
    ttll_priv  = Sample('ttll_multidim')
    tllq_priv  = Sample('tllq_multidim')
    ttlnu_priv = Sample('ttlnu_multidim')

    samples = []

    path = 'awightma/analysisTrees/special/private_sgnl_reprocFullWF-a29_NoStreaming_2019_11_08/v1'
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b1')
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b2')
    tHq_priv.addDirectory(HADOOP_DIR,path,'tHq_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b1')
    ttll_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b2')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b1')
    ttlnu_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b2')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b2')
    tllq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b3')

    samples.extend([ttH_priv,tHq_priv,ttll_priv,tllq_priv,ttlnu_priv])
    return samples

# Significantly larger stats version of a31, with all ttH events using the old starting point
def anatest32_samples(private_sgnl,central_sgnl,central_bkgd,data):
    logger.info("include private_sgnl: {}".format(private_sgnl))
    logger.info("include central_sgnl: {}".format(central_sgnl))
    logger.info("include central_bkgd: {}".format(central_bkgd))
    logger.info("include data: {}".format(data))

    SingleElectron = Sample('SingleElectron')
    SingleMuon = Sample('SingleMuon')
    DoubleMuon = Sample('DoubleMuon')
    DoubleEG   = Sample('DoubleEG')
    MuonEG     = Sample('MuonEG')

    ttZ_central = Sample('ttZ')
    ttW_central = Sample('ttW')
    ttH_central = Sample('ttH')
    tZq_central = Sample('tZq')
    tHq_central = Sample('tHq')

    ttGJets_central = Sample('ttGJets')
    ZZZ_central = Sample('ZZZ')
    WZZ_central = Sample('WZZ')
    WWZ_central = Sample('WWZ')
    WWW_central = Sample('WWW')
    ZZ_central  = Sample('ZZ')
    WZ_central  = Sample('WZ')
    WW_central  = Sample('WW')

    ttH_priv = Sample('ttH_multidim')
    tHq_priv = Sample('tHq_multidim')
    ttZ_priv = Sample('ttll_multidim')
    tZq_priv = Sample('tllq_multidim')
    ttW_priv = Sample('ttlnu_multidim')

    samples = []

    # Data samples
    path = 'awightma/analysisTrees/data2017_2019_10_21/v1'
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017B')
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017C')
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017D')
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017E')
    SingleElectron.addDirectory(HADOOP_DIR,path,'SingleElectron_Run2017F')

    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017B')
    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017C')
    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017D')
    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017E')
    SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017F')
    # SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017H')    # Has no data

    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017B')
    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017C')
    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017D')
    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017E')
    DoubleMuon.addDirectory(HADOOP_DIR,path,'DoubleMuon_Run2017F')

    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017B')
    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017C')
    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017D')
    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017E')
    DoubleEG.addDirectory(HADOOP_DIR,path,'DoubleEG_Run2017F')

    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017B')
    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017C')
    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017D')
    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017E')
    MuonEG.addDirectory(HADOOP_DIR,path,'MuonEG_Run2017F')

    if data: samples.extend([SingleElectron,SingleMuon,DoubleMuon,DoubleEG,MuonEG])

    # Central samples (same as ana26)
    path = 'awightma/analysisTrees/central_sgnl_2019_10_11/v1'
    tHq_central.addDirectory(HADOOP_DIR,path,'tHq') # Excluding this from the list for now
    ttW_central.addDirectory(HADOOP_DIR,path,'ttW')
    ttZ_central.addDirectory(HADOOP_DIR,path,'ttZ')

    # Updated to 'new_pmx' version (Note: The ttGJets sample failed horrifically)
    path = 'awightma/analysisTrees/special/central_ttH-WW-WZ-ttGJets_new_pmx_2019_10_21/v3'
    ttH_central.addDirectory(HADOOP_DIR,path,'ttH')

    # Used the correct 4f PDF for the PDFUP/PDFDOWN unc. templates
    path = 'awightma/analysisTrees/special/tZq_2019_10_17_fixPDF/v2'
    tZq_central.addDirectory(HADOOP_DIR,path,'tZq')

    if central_sgnl: samples.extend([ttW_central,ttZ_central,tZq_central,ttH_central])

    path = 'awightma/analysisTrees/central_bkgd_2019_10_12/v1'
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets')
    ttGJets_central.addDirectory(HADOOP_DIR,path,'ttGJets_ext')
    ZZZ_central.addDirectory(HADOOP_DIR,path,'ZZZ')
    WZZ_central.addDirectory(HADOOP_DIR,path,'WZZ')
    WWZ_central.addDirectory(HADOOP_DIR,path,'WWZ')
    WWW_central.addDirectory(HADOOP_DIR,path,'WWW')
    WW_central.addDirectory(HADOOP_DIR,path,'WW')

    # Updated to 'new_pmx' version (Note: The ttGJets sample failed horrifically)
    path = 'awightma/analysisTrees/special/central_ttH-WW-WZ-ttGJets_new_pmx_2019_10_21/v3'
    WZ_central.addDirectory(HADOOP_DIR,path,'WZ')
    ZZ_central.addDirectory(HADOOP_DIR,path,'ZZ')

    if central_bkgd: samples.extend([ttGJets_central,ZZZ_central,WZZ_central,WWZ_central,WWW_central,WW_central,WZ_central,ZZ_central])

    path = 'awightma/analysisTrees/private_sgnl_2019_12_02/v3'
    ttH_priv.addDirectory(HADOOP_DIR,path,'ttH_multidim_b1')
    tHq_priv.addDirectory(HADOOP_DIR,path,'tHq_multidim_b1')
    ttZ_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b1')
    ttZ_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b2')
    ttZ_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b3')
    ttZ_priv.addDirectory(HADOOP_DIR,path,'ttll_multidim_b4')
    tZq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    tZq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b2')
    tZq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b3')
    tZq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b4')
    tZq_priv.addDirectory(HADOOP_DIR,path,'tllq_multidim_b5')
    ttW_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b1')
    ttW_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b2')
    ttW_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b3')
    ttW_priv.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b4')

    if private_sgnl: samples.extend([ttH_priv,tHq_priv,ttZ_priv,tZq_priv,ttW_priv])
    return samples

####################################################################################################
# Smaller subsets of all samples
####################################################################################################
def a28_NoStreamingV1():
    sdir2 = "private_sgnl_reprocFullWF-a28_NoStreaming_2019_11_08/v1"
    
    sub_dir = 'awightma/analysisTrees/special'
    ttH   = Sample('ttH_multidim')
    tHq   = Sample('tHq_multidim')
    ttll  = Sample('ttll_multidim')
    tllq  = Sample('tllq_multidim')
    ttlnu = Sample('ttlnu_multidim')

    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b1')
    tHq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tHq_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b2')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b1')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b2')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b1')
    samples = [ttH,ttlnu,ttll,tllq,tHq]

    return samples

def a28_NoStreamingV2():
    sdir2 = "private_sgnl_reprocFullWF-a28_NoStreaming_2019_11_09/v1"
    
    sub_dir = 'awightma/analysisTrees/special'
    ttH   = Sample('ttH_multidim')
    tHq   = Sample('tHq_multidim')
    ttll  = Sample('ttll_multidim')
    tllq  = Sample('tllq_multidim')
    ttlnu = Sample('ttlnu_multidim')

    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b1')
    tHq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tHq_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b2')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b1')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b2')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b1')
    samples = [ttH,ttlnu,ttll,tllq,tHq]

    return samples

def a29_NoDupesV1():
    sdir2 = "private_sgnl_reprocFullWF-a29_NoDuplicates_2019_11_11/v2"

    sub_dir = 'awightma/analysisTrees/special'
    ttH   = Sample('ttH_multidim')
    tHq   = Sample('tHq_multidim')
    ttll  = Sample('ttll_multidim')
    tllq  = Sample('tllq_multidim')
    ttlnu = Sample('ttlnu_multidim')

    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b1')
    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b2')
    tHq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tHq_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b2')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b1')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b2')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b3')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b1')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b2')
    samples = [ttH,ttlnu,ttll,tllq,tHq]

    return samples

def a29_NoDupesV2():
    sdir2 = "private_sgnl_reprocFullWF-a29_NoDuplicates_2019_11_18/v1"

    sub_dir = 'awightma/analysisTrees/special'
    ttH   = Sample('ttH_multidim')
    tHq   = Sample('tHq_multidim')
    ttll  = Sample('ttll_multidim')
    tllq  = Sample('tllq_multidim')
    ttlnu = Sample('ttlnu_multidim')

    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b1')
    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b2')
    tHq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tHq_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b2')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b1')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b2')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b3')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b1')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b2')
    samples = [ttH,ttlnu,ttll,tllq,tHq]

    return samples

def HanV4SMCheck():
    sdir2 = "ttXJet-tXq4f-HanV4SMCheck_b1-b2_2019_11_18/v1"

    sub_dir = 'awightma/analysisTrees/special'
    ttH   = Sample('ttH_multidim')
    tHq   = Sample('tHq_multidim')
    ttll  = Sample('ttll_multidim')
    tllq  = Sample('tllq_multidim')
    ttlnu = Sample('ttlnu_multidim')

    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b1')
    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b2')
    tHq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tHq_multidim_b1')
    tHq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tHq_multidim_b2')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b2')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b1')
    tllq.addDirectory(HADOOP_DIR,sub_dir,sdir2,'tllq_multidim_b2')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b1')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b2')
    samples = [ttH,ttlnu,ttll,tllq,tHq]

    return samples

def HanOrigSMCheck():
    sdir2 = "ttXJet_HanModelOriginal_SMCheck_2019_11_18/v1"
    
    sub_dir = 'awightma/analysisTrees/special'
    ttH   = Sample('ttH_multidim')
    tHq   = Sample('tHq_multidim')
    ttll  = Sample('ttll_multidim')
    tllq  = Sample('tllq_multidim')
    ttlnu = Sample('ttlnu_multidim')

    # No tZq or tHq currently
    ttH.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttH_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttll_multidim_b1')
    ttlnu.addDirectory(HADOOP_DIR,sub_dir,sdir2,'ttlnu_multidim_b1')
    samples = [ttH,ttlnu,ttll]

    return samples

def tllq_SM_Q2RF_Check():
    path = 'awightma/analysisTrees/special/private_tllq_SM_Q2RF-check_2020_01_13/v1'
    tllq = Sample('tllq_multidim')

    tllq.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    tllq.addDirectory(HADOOP_DIR,path,'tllq_multidim_b2')

    samples = [tllq]
    return samples

def private_sgnl_SM_ISR_FSR_Systematics():
    path = 'awightma/analysisTrees/special/private_sgnl_SM_ISR-FSR-qCut_Systematics_2020_01_16/v2'

    ttH   = Sample('ttH_multidim')
    tHq   = Sample('tHq_multidim')
    ttll  = Sample('ttll_multidim')
    tllq  = Sample('tllq_multidim')
    ttlnu = Sample('ttlnu_multidim')

    ttH.addDirectory(HADOOP_DIR,path,'ttH_multidim_b1')
    tHq.addDirectory(HADOOP_DIR,path,'tHq_multidim_b1')
    ttll.addDirectory(HADOOP_DIR,path,'ttll_multidim_b1')
    tllq.addDirectory(HADOOP_DIR,path,'tllq_multidim_b1')
    ttlnu.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_b1')

    samples = [ttH,tHq,ttll,tllq,ttlnu]
    return samples

def private_sgnl_SM_qCutUp_Systematics():
    path = 'awightma/analysisTrees/special/private_sgnl_SM_ISR-FSR-qCut_Systematics_2020_01_16/v2'

    ttH   = Sample('ttH_multidim')
    ttll  = Sample('ttll_multidim')
    ttlnu = Sample('ttlnu_multidim')

    ttH.addDirectory(HADOOP_DIR,path,'ttH_multidim_qCutUp_b1')
    ttll.addDirectory(HADOOP_DIR,path,'ttll_multidim_qCutUp_b1')
    ttlnu.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_qCutUp_b1')

    samples = [ttH,ttll,ttlnu]
    return samples

def private_sgnl_SM_qCutDown_Systematics():
    path = 'awightma/analysisTrees/special/private_sgnl_SM_ISR-FSR-qCut_Systematics_2020_01_16/v2'

    ttH   = Sample('ttH_multidim')
    ttll  = Sample('ttll_multidim')
    ttlnu = Sample('ttlnu_multidim')

    ttH.addDirectory(HADOOP_DIR,path,'ttH_multidim_qCutDown_b1')
    ttll.addDirectory(HADOOP_DIR,path,'ttll_multidim_qCutDown_b1')
    ttlnu.addDirectory(HADOOP_DIR,path,'ttlnu_multidim_qCutDown_b1')

    samples = [ttH,ttll,ttlnu]
    return samples

####################################################################################################

# These are analysis trees located in /scratch365/awightma for use when hadoop is unavailable
def tmp_geoff_samples(private_sgnl,central_sgnl,central_bkgd,data):
    logger.info("include private_sgnl: {}".format(private_sgnl))
    logger.info("include central_sgnl: {}".format(central_sgnl))
    logger.info("include central_bkgd: {}".format(central_bkgd))
    logger.info("include data: {}".format(data))

    SingleElectron = Sample('SingleElectron')
    SingleMuon = Sample('SingleMuon')
    DoubleMuon = Sample('DoubleMuon')
    DoubleEG   = Sample('DoubleEG')
    MuonEG     = Sample('MuonEG')

    samples = []

    # Data samples
    path = '/scratch365/awightma/temp_workflow_steps/geoff_lobster_outputs/lobster_trees__EFT_test_20_2_19_data_take2'
    SingleElectron.addDirectory(path,'SingleElectron_Run2017B')
    SingleElectron.addDirectory(path,'SingleElectron_Run2017C')
    SingleElectron.addDirectory(path,'SingleElectron_Run2017D')
    SingleElectron.addDirectory(path,'SingleElectron_Run2017E')
    SingleElectron.addDirectory(path,'SingleElectron_Run2017F')

    SingleMuon.addDirectory(path,'SingleMuon_Run2017B')
    SingleMuon.addDirectory(path,'SingleMuon_Run2017C')
    SingleMuon.addDirectory(path,'SingleMuon_Run2017D')
    SingleMuon.addDirectory(path,'SingleMuon_Run2017E')
    SingleMuon.addDirectory(path,'SingleMuon_Run2017F')
    # SingleMuon.addDirectory(HADOOP_DIR,path,'SingleMuon_Run2017H')    # Has no data

    DoubleMuon.addDirectory(path,'DoubleMuon_Run2017B')
    DoubleMuon.addDirectory(path,'DoubleMuon_Run2017C')
    DoubleMuon.addDirectory(path,'DoubleMuon_Run2017D')
    DoubleMuon.addDirectory(path,'DoubleMuon_Run2017E')
    DoubleMuon.addDirectory(path,'DoubleMuon_Run2017F')

    DoubleEG.addDirectory(path,'DoubleEG_Run2017B')
    DoubleEG.addDirectory(path,'DoubleEG_Run2017C')
    DoubleEG.addDirectory(path,'DoubleEG_Run2017D')
    DoubleEG.addDirectory(path,'DoubleEG_Run2017E')
    DoubleEG.addDirectory(path,'DoubleEG_Run2017F')

    MuonEG.addDirectory(path,'MuonEG_Run2017B')
    MuonEG.addDirectory(path,'MuonEG_Run2017C')
    MuonEG.addDirectory(path,'MuonEG_Run2017D')
    MuonEG.addDirectory(path,'MuonEG_Run2017E')
    MuonEG.addDirectory(path,'MuonEG_Run2017F')

    if data: samples.extend([SingleElectron,SingleMuon,DoubleMuon,DoubleEG,MuonEG])

    return samples

def main():
    overwrite = False
    timestamp_directory = True
    sub_index = False    # Will also create an inputfile for each directory of a sample
    is_test = True      # In case we don't want to be making a bunch of useless output directories
    dir_name = 'scratch365_geoff_inputfiles_full_data'

    if is_test:
        dir_name = 'testing'
        overwrite = True
        timestamp_directory = False

    if timestamp_directory:
        dir_name = '{tstamp}_{base}'.format(tstamp=TIMESTAMP1,base=dir_name)

    out_dir = os.path.join(INPUTFILES_DIR,dir_name)
    if os.path.exists(out_dir):
        if not overwrite:
            err = "[ERROR] Output directory already exists and overwrite set to false!"
            raise RuntimeError(err)
    else:
        print "Making output directory: {}".format(out_dir)
        os.makedirs(out_dir)

    log_file = os.path.join(out_dir,'out.log')
    outlog = logging.FileHandler(filename=log_file,mode='w')
    outlog.setLevel(logging.DEBUG)
    outlog.setFormatter(frmt)

    logger.addHandler(outlog)

    logger.info("Start: {tstamp}".format(tstamp=TIMESTAMP3))
    logger.info("Output Directory: {path}".format(path=out_dir))
    logger.info("Logging output to: {path}".format(path=log_file))

    logger.info("Overwrite: {}".format(overwrite))
    logger.info("Timestamp: {}".format(timestamp_directory))
    logger.info("Sub-Index: {}".format(sub_index))
    logger.info("Testing: {}".format(is_test))

    # Dictionary for easily specifying which samples to be included
    kwargs = {
        'private_sgnl': False,
        'central_sgnl': False,
        'central_bkgd': False,
        'data': True,
    }

    # samples = private_samples()
    # samples = legacy_geoff_samples(**kwargs)
    # samples = anatest25_samples()
    # samples = anatest26_samples()
    # samples = anatest27_samples()
    # samples = anatest29_samples()
    # samples = anatest31_samples()
    # samples = anatest32_samples(**kwargs)
    # samples = tmp_geoff_samples(**kwargs)

    samples = private_sgnl_SM_ISR_FSR_Systematics()
    # samples = private_sgnl_SM_qCutUp_Systematics()
    # samples = private_sgnl_SM_qCutDown_Systematics()

    # samples = a29_NoDupesV2()

    logger.info("-"*100)
    for idx,s in enumerate(samples):
        outf = 'inputfiles__{name}.txt'.format(name=s.name())
        logger.info("[{}/{}] Making file: {fn}".format(idx+1,len(samples),fn=outf))
        s.write(out_dir,sub_index=sub_index,skip_copy=is_test)
        logger.info("-"*100)
    logger.info("Finished!")

if __name__ == "__main__":
    main()
    logger.info('Logger shutting down!')
    logging.shutdown()
